---
title: "BIOS735 Group 6 Final project"
author: "Group 6"
date: "4/15/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pkgs <- list('tidyverse', 'table1', 'e1071', 'caret', 'ROCR')
lapply(pkgs, require, character.only = T)
```

## Data pre-processing

We'll use bank-full.csv and randomly select 30% data as our full working dataset.

(I copied Shuang's code here. Maybe it needs further simplification.)

```{r}
## read data
dat_full <- read_delim("bank-full.csv",
           delim=';')

## overview
table1(~.|y, data = dat_full)

#check null
sum(is.na(dat_full)) #0
summary(dat_full)

dat_full %>% group_by(education) %>% summarize(count=n())

#visualization
#library(corrplot)
#new <- dat.downsampled[,c(1,6,10,12,13,14,15)]
#res <- cor(new)
#corrplot(res,type = "upper", order = "hclust", 
#         tl.col = "black", tl.srt = 45)

## combine the labels
library(car)

unknown <- c('unknown')
no_income <- c('unemployed','student')
low_income <- c('blue-collar','housemaid','retired','self-employed')
high_income <- c('admin.','entrepreneur','management','services','technician')



spring <- c('mar','feb','jan')
summer <- c('apr','may','jun')
autumn <- c('jul','aug','sep')
winter <- c('nov','oct','dec')



dat_full$job<-recode(dat_full$job,"unknown='unknown'; no_income='no_income';
                      low_income = 'low_income'; high_income = 'high_income'")

dat_full$month<-recode(dat_full$month,"spring='1_2_3'; summer='4_5_6';
                      autumn = '7_8_9'; winter = '10_11_12'")

# select variables (data pre-processing: keep consistent)
#drop pdays
dat = dat_full %>% select(-pdays)

scale2sd <- function(x) {
  (x - mean(x) )/ (2*sd(x))
}


```

```{r}
dat.working <- dat %>% 
  mutate(age = scale2sd(age),
         campaign = scale2sd(campaign),
         balance = scale2sd(balance),
         duration = scale2sd(duration),
         previous = scale2sd(previous)) %>% # put continuous vars on same scale as indicators
  mutate_if(is.character, as.factor) %>% # convert character to factor
  slice_sample(prop = 0.3) # select subset

summary(dat.working)

# check data imbalance
dat.working %>% group_by(y) %>% summarize(count=n())


# #downsampling
# set.seed(100)
# dat.downsampled <- downSample(x=dat.working[,-ncol(dat.working)], y=dat.working$y)

# select training and testing data
set.seed(100)
y <- dat.working$y
index <- createDataPartition(
  y,
  times = 1,
  p = 0.8,
  list = F,
  groups = min(5, length(y))
)
dat_train <- dat.working[index,]
dat_test <- dat.working[-index,]

```

visualizations
```{r}
library(ggplot2)
ggplot(dat_full, aes(x = y, y = age)) + xlab('subscribed')+
  geom_boxplot(color = "steelblue")

ggplot(dat_full, aes(x = y, y = balance)) + xlab('subscribed')+
  geom_boxplot(color = "steelblue")

ggplot(dat.working, aes(x = age, fill = y)) + geom_density(alpha = 0.5,position = 'stack')

```



```{r}
library(treemapify)
ggplot(dat_full, aes(
  area =age , fill = job,label = job
)) +
  geom_treemap() +
  geom_treemap_text(grow = T, reflow = T, colour = "black")
```



```{r}

```


## SVM

We'll use 5-fold cross validation to select the tuning parameter "cost".

```{r}
# SVM: tune cost parameter 
set.seed(100)
start = Sys.time()
tune.out=tune.svm(y~., data=dat_train, kernel ="linear", type='C-classification', 
                  cost=c(0.001, 0.01, 0.1, 1, 5),
                  tunecontrol = tune.control(sampling = "cross", cross = 5),
                  probability=TRUE)
Sys.time() - start # 4 min

summary(tune.out)
best.linear = tune.out$best.model

# prediction on training dataset
svm.pred <- predict(best.linear, dat_train %>% select(!'y'), probability=TRUE)
confusionMatrix(svm.pred, dat_train$y, positive = 'yes')$byClass

# on testing dataset
svm.pred.test <- predict(best.linear, dat_test %>% select(!'y'), probability=TRUE)
confusionMatrix(svm.pred.test, dat_test$y, positive = 'yes')$byClass

# ROC curve using ROCR package
pr <- prediction(attr(svm.pred.test, "probabilities")[,2], dat_test$y)
prf <- performance(pr, measure="tpr", x.measure="fpr")
plot(prf, col = "lightsteelblue", print.auc=TRUE, print.auc.x = 0.5, print.auc.y = 0.3)
lines(x = c(0,1), y = c(0,1),col="black")
performance(pr, measure = 'auc')@y.values %>% print() # AUC

```

## Ridge logistic regression

(Here I am using glmnet as a substitute)

```{r}
# ridge regression
library(glmnet)
x <- model.matrix(y~., dat_train)[,-1]
y <- ifelse(dat_train$y == "yes", 1, 0)

set.seed(100)
cv.ridge <- cv.glmnet(x, y, alpha = 0, family = "binomial")

model <- glmnet(x, y, family = "binomial", alpha = 0, lambda = cv.ridge$lambda.min)
coef(model)

x.test <- model.matrix(y~., dat_test)[,-1]
probabilities <- model %>% predict(newx = x.test, type='response')

# Model performance
predicted.classes <- ifelse(probabilities > 0.5, "yes", "no")
confusionMatrix(as_factor(predicted.classes), dat_test$y, positive = 'yes')$byClass

# ROC curve using ROCR package
pr.2 <- prediction(probabilities, dat_test$y)
prf.2 <- performance(pr.2, measure="tpr", x.measure="fpr")
plot(prf.2, col = "goldenrod", print.auc=TRUE, print.auc.x = 0.5, print.auc.y = 0.3)
plot(prf, add=TRUE, col = "lightsteelblue", print.auc=TRUE, print.auc.x = 0.5, print.auc.y = 0.3)
lines(x = c(0,1), y = c(0,1),col="black")
legend("bottom",
       legend=c("ridge", "svm"),
       col=c("goldenrod", "lightsteelblue"),
       lwd=4, cex =0.8, xpd = TRUE, horiz = TRUE)

performance(pr.2, measure = 'auc')@y.values %>% print()
```