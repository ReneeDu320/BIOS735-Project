---
title: "BIOS735 Group 6 Final project"
author: "Group 6"
date: "4/30/2022"
output: html_document
bibliography: references.bib  
csl: biomed-central.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE)

# load packages
pkgs <- list('tidyverse', 'table1', 'e1071', 'caret', 'ROCR', 
             'devtools', 'OptimalCutpoints', 'car', 'WeightSVM', 
             'knitr', 'kableExtra', 'gridExtra')
lapply(pkgs, require, character.only = T)
load_all('./final')

# set ggplot theme
theme_set(theme_bw())

# run_from_scratch = FALSE uses saved model objects
# set to TRUE if want to run from scratch
run_from_scratch = FALSE

# read data
dat_full <- read_delim("bank-full.csv", delim=';')
```

## Introduction

The topic of our project is to improve the performance in telemarketing using predictive models. Telemarketing is a method of marketing directly to customers via phone or the Internet in order to sell goods or services. The telemarketing industry has a market size of $23.4 bn in 2022 and is still expecting a 2% growth in market [size](https://www.ibisworld.com/industry-statistics/market-size/telemarketing-call-centers-united-states/). A central challenge in telemarketing is in targeting individuals to sell goods or services. It would be costly to indiscriminately canvas everyone, but it would also be undesirable to target a random subset of the population as many potential sales could be lost. Thus, there is great interest in tools to identify the groups of individuals for whom telemarketing would likely result in sales and the groups of whom telemarketing is unlikely to be successful. Then, the firm could save resources by ignoring those in the latter group while not losing out on potential sales @marshall1988successfully. 

In this project, we analysed a telemarketing dataset from a Portuguese retail bank [dataset](https://archive.ics.uci.edu/ml/datasets/bank+marketing) @moro2014data, which contains information on clients who were contacted by phone call to sell term deposit subscriptions (also known as certificate of deposits) and the outcome of the contact, which is a binary indicator for if the client subscribed a term deposit, which we call a "success". 

The goal of this project was to build a classifier using the covariates listed in Table 1 to predict whether or not that client subscribed a term deposit. We chose F1 Score as the criterion to evaluate model performance, with accuracy, Kappa, sensitivity, and specificity as secondary metrics. We used logistic regression with a ridge penalty and support vector machines (SVM) as the candidate models. The ridge logistic regression model was implemented from scratch while pre-existing R packages were used for SVM. An R package was developed that can be used to replicate our analysis. All analysis materials are in our [Github](https://github.com/ReneeDu320/BIOS735-Project) page. 


```{r DataDescription, results = "asis", echo = FALSE, message = FALSE}

tex2markdown <- function(texstring) {
  writeLines(text = texstring,
             con = myfile <- tempfile(fileext = ".tex"))
  texfile <- pandoc(input = myfile, format = "html")
  cat(readLines(texfile), sep = "\n")
  unlink(c(myfile, texfile))
}

textable <- "
\\begin{table}[ht]
\\centering
\\caption{Table 1: Description of variables in telemarketing data set}
\\begin{tabular}{| l | l |}
\\hline
Feature Name & Description   \\\\
\\hline
age          & numeric  \\\\
job          & type of job (categorical) \\\\
marital      & marital status (categorical: married, divorced, single) \\\\
education    & (categorical: unknown,secondary, primary, tertiary) \\\\
default      & has credit in default? (binary: yes, no)\\\\
balance      & average yearly balance, in euros (numeric)\\\\
housing      & has housing loan? (binary: yes, no)\\\\
loan         & has personal loan? (binary: yes, no)\\\\
contact      & contact communication type (categorical: unknown, telephone, cellular)\\\\
day          & last contact day of the month (numeric)\\\\
month        & last contact month of year (categorical: jan, feb, mar, ..., nov, dec)\\\\
duration     & last contact duration, in seconds (numeric)\\\\
campaign     & number of contacts performed during this campaign and for this client (numeric)\\\\
pdays        & number of days that passed by after last contacted from a previous campaign (numeric) \\\\
previous     & number of contacts performed before this campaign and for this client (numeric) \\\\
poutcome     & outcome of previous campaign (categorical: unknown, other, failure, success) \\\\
outcome      & has the client subscribed a term deposit? (binary: yes, no)                                                                                                               
\\end{tabular}
\\end{table}
"

tex2markdown(textable)
```

## Data description and pre-processing

There are four datasets available. We chose the dataset with 16 features and all 45,211 observations (called `bank-full.csv` on the source website). To ease computational burden, we randomly selected 20 percent of the data for our analysis dataset(`dat.working`), which includes 9,042 observations.

The features are described in Table 1 and summary statistics are computed and shown in Table 2. The features include some basic demographic and financial information on clients as well as some information on previous contacts. There were some missing values in the categorical features and these observations were treated as their own category, "unknown," and are shown in Table 2. We also collapsed months into quarters. The feature `pdays` was mostly missing (80%) so it was dropped from the analysis dataset. Additionally, the feature `duration` refers to the duration of the phone call that corresponds to the outcome variable and was therefore also dropped from the analysis dataset since the goal was to predict prospectively. 

Continuous variables were centered and scaled by two standard deviations to be put on approximately the same scale as categorical covariates @gelman. This transformation allows coefficients to be readily and easily interpreted while also allowing for each term to be penalized approximately the same. As a sensitivity check, regular standardization (centering and scaling by one standard deviation) was performed on all covariates and the analysis re-run for SVM, and we found no substantial differences in results. 

The binary outcome variable is highly imbalanced, with 7983 (88%) failures and 1059 (12%) successes. 


```{r DataPreprocessing, warning=FALSE}
# combine the labels
q1 <- c('mar','feb','jan')
q2 <- c('apr','may','jun')
q3 <- c('jul','aug','sep')
q4 <- c('nov','oct','dec')

#collape month into quarters
dat_full$month<-recode(dat_full$month,
                       "q1='q1'; q2='q2'; q3 = 'q3'; q4 = 'q4'")

# drop pdays and duration
dat = dat_full %>% select(-pdays, -duration)

# analysis dataset
set.seed(100)
dat.working <- dat %>% 
  mutate_if(is.character, as.factor) %>% # convert character to factor
  slice_sample(prop = 0.2) # select subset
```


```{r table1}
table1(~.|y, data = dat.working,
       caption = 'Table 2: Summary of features, by observed success/failure')
```

<br>

Figures 1-4 show some insights from the data. Figure 1 shows a density plot of age by subscription status. Most individuals in the data are between 25 and 60 years old. Additionally, relatively more individuals subcribed than not at the extremes of the age distribution.  

```{r AgePlot1}
ggplot(dat.working, aes(x = age, fill = y)) + 
  geom_density(alpha = 0.5,position = 'stack') + 
  theme_classic() +
  labs(title = 'Figure 1: Age density plot by subscribed status',
       fill = 'Subscribed',
       x = 'Age',
       y = 'Density')
```

Figure 2 shows the distribution of mean yearly balance over age group. Older age groups had a larger yearly balance, on average, compared to younger age groups, with the exception of the oldest age group. 

```{r AgePlot2, include=FALSE}
data_new = dat.working %>%
  select(age, balance)%>%
  mutate(age_group =  case_when(age < 20 ~ "<20",
                                
                           30 > age & age >= 20 ~ "20-30",
                           40> age & age >= 30 ~ "30-40",
                         50 > age & age >= 40 ~ "40-50",
                           60 > age & age >= 50 ~ "50-60",
                         70 > age & age >= 60 ~ "60-70",
                         80 > age & age >=70 ~ "70-80",
                         age >= 80 ~ "80-100"
                         
                         ))%>%
  group_by(age_group)%>%
  summarise(mean_balance = sum(balance)/n() )
```

```{r AgePlot3}
ggplot(data_new, aes( y=mean_balance, x=age_group)) + 
    geom_bar(position="dodge", stat="identity",fill = 'plum3') +
  labs(title = 'Figure 2: Mean yearly balance, averaged by age group',
       x = 'Age group',
       y = 'Mean yearly balance') + 
  theme_minimal()

```


Figure 3 shows the total number of campaigns by job type for subscribers and non-subscribers. It shows the amount of effort the bank pursued each occupation group. However, this plot may just reflect the number of each job type in the population and sample. Figure 4 shows the average number of campaigns, and we observe that the bank pursued each group by proportionally similar amounts. 


```{r CampaignPlot}
# unknown <- c('unknown')
# no_income <- c('unemployed','student')
# low_income <- c('blue-collar','housemaid','retired','self-employed')
# high_income <- c('admin.','entrepreneur','management','services','technician')
# dat_plot <- dat_full
# dat_plot$job<-recode(dat_plot$job,"unknown='unknown'; no_income='no_income';
#                       low_income = 'low_income'; high_income = 'high_income'")

dat_plot = dat.working %>%
  group_by(job, y) %>%
  summarize(avg_campaign = mean(campaign),
            tot_campaign = sum(campaign)) %>%
  ungroup()

ggplot(dat_plot, aes(fill=y, y=tot_campaign, x=reorder(job, tot_campaign, function(x) x[2]))) + 
  geom_bar(position='dodge', stat='identity', width=0.8) + 
  theme_classic() + 
  labs(fill = "Subscribed",
       title = 'Figure 3: Total number of campaigns sent for each job type',
       y = 'Total campaigns',
       x = 'Job type') + 
  coord_flip() +
  scale_fill_manual(values = c("burlywood1","darkolivegreen3"))

ggplot(dat_plot, aes(fill=y, y=avg_campaign, x=reorder(job, avg_campaign, function(x) x[2]))) + 
  geom_bar(position='dodge', stat='identity', width=0.8) + 
  theme_classic() + 
  labs(fill = "Subscribed",
       title = 'Figure 4: Average number of campaigns sent for each job type',
       y = 'Average campaigns',
       x = 'Job type') + 
  coord_flip() +
  scale_fill_manual(values = c("burlywood1","darkolivegreen3"))
```



```{r SelectTrain/test}

# put continuous vars on same scale as indicators
# put here b/c want summary table/figures to show on raw scale
dat.working = dat.working %>%
  mutate(age = scale2sd(age),
         campaign = scale2sd(campaign),
         balance = scale2sd(balance),
         previous = scale2sd(previous))

# select training and testing data
set.seed(100)
y <- dat.working$y
index <- createDataPartition(
  y,
  times = 1,
  p = 0.8,
  list = F,
  groups = min(5, length(y))
)
dat_train <- dat.working[index,]
dat_test <- dat.working[-index,]

```


## Methods

We chose logistic regression as our likelihood-based model since it allows for classification. We consider a ridge penalty here since penalization often improves prediction, which is the primary goal. A ridge penalty is chosen as opposed to a lasso penalty since the number of features is relatively few compared to the number of observations so feature selection may be unnecessary. 

Support vector machines were chosen for their flexibility and good out-of-box performance for classification. By fitting SVM using both linear and RBF kernels, we allow for flexibility in the relationship between the outcome and features. 

We held out 20 percent of the data as a test set for final model performance evaluation. The remaining 80 percent of the data was used as training data to select hyperparameters in the logistic regression and SVM models. We chose F1 score as our final metric for model comparison. F1 score is the harmonic mean of the precision and recall: $F1 = \frac{2(P \cdot R)}{P+R}$, where $P$ is the precision: $\frac{TP}{TP + FP}$, and $R$ is the recall of the classification model: $\frac{TP}{TP + FN}$, where $TP$ is the number of true positives, $FP$ is the number of false positives, and $FN$ is the number of false negatives.


### Logistic regression with ridge penalty

We develop the algorithm to fit a ridge logistic regression model by first introducing the logistic regression model without any penalization. Let $i=1, 2, \dots , n$ be the number of subjects. We observe the outcome $Y_{i}$ for subject $i$, and we observe the subject's covariates $x_{i}$.

Assume $y_{i}|x_{i}\sim Binomial(1, p_{i})$ for $i=1, ..., n$;
$logit(p_{i})=log(\frac{p_{i}}{1-p_{i}})=x_{i}^T\beta=\eta_{i}$ for $i=1, ..., n$. Thus, the log-likelihood function of $\beta$ for the logistic regression model is given by
\begin{equation}
	l_{n}(\beta)=\sum_{i=1}^{n}\left\{y_{i}x_{i}^T\beta-log(1+exp(x_{i}^T\beta)) \right\}
\end{equation}

We use iteratively re-weighted least squares (IRLS) to obtain the ML estimate $\hat{\beta}$ in this GLM model by

\begin{equation}
\begin{aligned}
	\beta^{(t+1)}&=\beta^{(t)}-l''(\beta^{(t+1)})^{-1}l'(\beta^{(t)})\\
	&=\beta^{(t)}+(X^TW^{(t)}X)^{-1}X^T(Y-p^{(t)})\\
	&=\beta^{(t)}+h^{(t)}
\end{aligned}
\end{equation}

where $p^{(t)}$ depends on $\beta^{(t)}$ and $W$ is a diagonal matrix with the $i$th diagonal element having value equal to $p_{i}(1-p_{i})$. $W^{(t)}$ is a function of $p^{(t)}$. Since logistic regression uses the canonical link function, the IRLS solution is equivalent to Newton-Raphson. 

If we regularize the model with an L2 penalty we get ridge logistic regression @zou2005regularization. The objective function is then the log-likelihood with the penalty term:

\begin{equation}
	-\left[\frac{1}{N}\sum_{i=1}^{n}\left\{y_{i}x_{i}^T\beta-log(1+exp(x_{i}^T\beta)) \right\}\right]+\lambda(||\beta||_{2}),
\end{equation}

where $\lambda$ is a tuning parameter that controls the amount of penalization. The IRLS solution to this problem is similar as before, and the ridge estimate of the coefficients, $\hat{\beta}_{ridge}$, are found by iteratively updating using the following:

$$
\beta^{(t+1)} = \beta^{(t)} + (X^TWX + \lambda I_p)^{-1}X^T(Y-p^{(t)}),
$$
where the $I_p$ is a $p$ by $p$ identity matrix.

We used Iteratively Reweighted Least Squares (IRLS) @holland1977robust to perform optimization, and select the tuning parameter $\lambda$ using grid search for the $\lambda$ that yields the minimum mean square error (MSE) from cross-validation.

Both logistic regression and ridge logistic regression algorithm using IRLS approach can be summarized as follows:
![IRLS algorithm](IRLS.png){width=75%}

Since (ridge) logistic regression models the probability of an outcome, to get class predictions, a cutoff probability must be specified. To decide the optimal cutoff probability for ridge logistic regression, we used the cutoff that maximizes F1 score. 


### SVM

Support vector machines (SVM) @cortes1995support are a popular machine learning method for binary classification. SVM classifies observations by separating the feature space into two classes according to some decision boundary form with the goal of maximizing the number of correct classifications while allowing for some mis-classifications for identifiability and to prevent over-fitting. The form of the decision boundary is controlled by choice of the kernel function. We fitted SVM models using linear and radial kernels. 

Formally, SVM solves the following Lagrangian dual

\begin{equation}
	\begin{split}
		\max_{\substack{\alpha}}  \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i, j = 1}^{n} y_i y_j \alpha_i \alpha_j K(x_i, x_j) \\
		\text{s.t. } 0 \leq \alpha_i \leq C, i=1,\dots, n \text{ and } \sum_{i=1}^{n} \alpha_i y_i = 0
	\end{split}
\end{equation}

where $y \in \{-1, 1\}$ are the classification labels, $\alpha$ are the Lagrangian multipliers, $C$ is a tuning parameter chosen by cross-validation, and $K(x, x')$ represents the kernel function. The linear kernel corresponds to the dot product between $x$ and $x'$, and the radial kernel corresponds to $K(x, x') = \exp\left(  \gamma ||x-x'||^2 \right)$ where $\gamma$ is a tuning parameter that will be chosen by cross-validation. 

Since SVM is not good at handling class imbalance in the sense that it favors the majority class, we performed weighted SVM instead. In our dataset, the majority class are the negatives i.e. $Y=-1$. For the class $Y=-1$, the weight equals $\frac{1}{n} \sum_{i=1}^n I(Y_i=1)$ and for the class $Y=1$, the weight equals $1-\frac{1}{n} \sum_{i=1}^n I(Y_i=1)$. Thus, we down-weighted the majority class and up-weighted the minority class.

We fit SVM using the R packages `e1071` and `WeightSVM`.  


### Model evaluation

We used 5-fold cross validation to select tuning parameters for both logistic regression and SVM. For logistic regression with ridge penalty, there is a hyperparameter, $\lambda$ in our notation above, which controls the amount of penalization. We compare 10 different values of $\lambda$, $\exp(\{-5, -4.33, \dots, 1 \})$ and chose the $\lambda$ which minimized the mean-squared error. 

For SVM, we used both a linear kernel and a radial basis function (RBF) kernel. SVM with a linear kernel contains a penalization hyperparameter $C$, and SVM with an RBF kernel contains a penalization hyperparameter $C$ as well as a tuning parameter $\gamma$ which accounts for the smoothness of the decision boundary and controls the variance of the model. For the linear kernel we test $C=(0.001, 0.01, 0.1, 1, 5)$ and for the RBF kernel we test all combinations of $C=(0.001, 0.01, 0.1, 1)$ and $\gamma=(0.03125, 0.12500, 0.50000, 2.00000)$. The error function to minimize was the mis-classification rate. 


```{r FitRidge}
# fitting ridge logistic regression
# get data into format for logistic regression
x <- model.matrix(y~., dat_train)[,-1]
x.test <- model.matrix(y~., dat_test)[,-1]
y2 <- ifelse(dat_train$y == "yes", 1, 0)

# penalty parameters to test (on log-scale)
# NOTE: should change function argument to make clear it is on log-scale e.g. name it loglambda
lambdas = seq(-5,1,length.out=10)

# fit models using cv (read from file to save computation time)
if (run_from_scratch) {
  # set seed
  set.seed(100)

  # takes about 13 mins
  cv.ridgeLR <- cv.glm.logit.ridge(x, y2, lambdas, fold=5)
  saveRDS(cv.ridgeLR, 'cv_ridge.rds')
  
} else {
  cv.ridgeLR = readRDS('cv_ridge.rds')
}

# best ridge logistic reg model
model.ridge <- glm.logit.ridge(x, y2, lambda = cv.ridgeLR$lambda.min)
probs.ridge.train <- predict.logistic(model.ridge, x)$prob

# get probability predictions on test set
probs.ridge.test <- predict.logistic(model.ridge, x.test)$prob

# ROC curve using ROCR package
pr.ridge.test <- prediction(probs.ridge.test, dat_test$y)
prf.ridge.test <- performance(pr.ridge.test, measure="tpr", x.measure="fpr")

# auc
ridge.auc = performance(pr.ridge.test, measure = 'auc')@y.values[[1]]
```


```{r RidgePerformance}
# Model performance - select logistic cutoff using training data
pr.ridge.train <- prediction(predict.logistic(model.ridge, x)$prob, dat_train$y)

# get sensitivity
sens <- data.frame(x=unlist(performance(pr.ridge.train, "sens")@x.values), 
                   y=unlist(performance(pr.ridge.train, "sens")@y.values))
# get specificity
spec <- data.frame(x=unlist(performance(pr.ridge.train, "spec")@x.values), 
                   y=unlist(performance(pr.ridge.train, "spec")@y.values))

# Select logistic cutoff using training data
# get precision
prec <- data.frame(x=unlist(performance(pr.ridge.train, "prec")@x.values), 
                   y=unlist(performance(pr.ridge.train, "prec")@y.values))
prec <- prec[-1,]

# get recall
rec <- data.frame(x=unlist(performance(pr.ridge.train, "rec")@x.values), 
                   y=unlist(performance(pr.ridge.train, "rec")@y.values))
rec <- rec[-1,]

cutoff <- prec$x[which(1/(rec$y)+1/(prec$y)==min(1/(rec$y)+1/(prec$y)))]
```

```{r WeightedSVM}
# weights for svm
w = rep(mean(y2), nrow(dat_train))
w = ifelse(y2==1, 1-w, w)

# Linear SVM
if (run_from_scratch) {
  
  # set seed
  set.seed(100)
  
  # do CV and select best model
  best.linear = best.tune_wsvm(train.x=x, train.y=dat_train$y, weight=w, ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5)),
                             tunecontrol=tune.control(sampling='cross', cross=5),
                             kernel='linear', scale=FALSE)
  
  # save
  saveRDS(best.linear, 'cv_sim_lin.rds')
  
} else {
  best.linear = readRDS('cv_sim_lin.rds')
}

# Radial SVM
if (run_from_scratch) {
  
  # set seed
  set.seed(100)
  
  # do CV and select best model
  best.radial = best.tune_wsvm(train.x=x, train.y=dat_train$y, weight=w,
                             ranges=list(cost=c(0.001, 0.01, 0.1, 1),
                                         gamma=2^seq(-5,2,by=2)),
                             tunecontrol=tune.control(sampling='cross', cross=5),
                             kernel='radial', scale=FALSE)
  
  # save
  saveRDS(best.radial, file = "radial_svm.RDS")
  
} else {
  best.radial = readRDS('radial_svm.RDS')
}
```


## Results

The chosen $\lambda$ for ridge logistic regression was 1.4. For the SVM models, the chosen cost hyperparameter for the linear kernel was 0.1, and for the RBF kernel, the chosen cost and $\gamma$ were 1 and 0.125, respectively. The numbers of support vectors are 5368 and 5497, respectively, for linear and kernel SVM.

For ridge logistic regression, the plot of sensitivity and specificity as a function of cutoff probability is shown in Figure 5 The optimal cutoff probability is 0.202, which maximizes F1 as stated in the Methods section. Figure 6 shows the ROC curve for ridge logistic regression model in red (black is a reference line for the worst case), and the area under the ROC curve was computed to be 0.73. 


```{r ResultRidge1}
# plot sensitivity and specificity as function of cost
sens %>% ggplot(aes(x,y)) + 
  geom_line() + 
  geom_line(data=spec, aes(x,y,col="red")) +
  scale_y_continuous(sec.axis = sec_axis(~., name = "Specificity")) +
  labs(x='Cutoff', y="Sensitivity",
       title='Figure 5: Sensitivity and specificity for varying cutoffs \nof ridge logistic regression model') +
  theme(axis.title.y.right = element_text(colour = "red"), legend.position="none")
```

```{r ResultAll1}

# train data; ridge
ridge.pred.train <- ifelse(probs.ridge.train > cutoff, "yes", "no")
ridge.train.res = confusionMatrix(factor(ridge.pred.train, levels = c('no', 'yes')), dat_train$y, positive = 'yes')$byClass

# train data; svm linear
svm.pred.lin.train <- predict(best.linear)
svm.lin.train.res = confusionMatrix(svm.pred.lin.train, dat_train$y, positive = 'yes')$byClass

# train data; svm radial
svm.pred.rad.train <- predict(best.radial)
svm.rad.train.res = confusionMatrix(svm.pred.rad.train, dat_train$y, positive = 'yes')$byClass

# test data; ridge
ridge.pred.test <- ifelse(probs.ridge.test > cutoff, "yes", "no")
ridge.test.res = confusionMatrix(factor(ridge.pred.test, levels = c('no', 'yes')), dat_test$y, positive = 'yes')$byClass

# test data; svm linear
svm.pred.lin.test <- predict(best.linear, x.test)
svm.lin.test.res = confusionMatrix(svm.pred.lin.test, dat_test$y, positive = 'yes')$byClass

# test data; svm radial
svm.pred.rad.test <- predict(best.radial, x.test)
svm.rad.test.res = confusionMatrix(svm.pred.rad.test, dat_test$y, positive = 'yes')$byClass


# put it all into a dataframe
pred_metrics = tibble(Metric = names(ridge.train.res),
                      `Ridge logistic, train` = ridge.train.res,
                      `SVM linear kernel, train` = svm.lin.train.res,
                      `SVM radial kernel, train` = svm.rad.train.res,
                      `Ridge logistic, test` = ridge.test.res,
                      `SVM linear kernel, test` = svm.lin.test.res,
                      `SVM radial kernel, test` = svm.rad.test.res)
```



```{r ResultRidge2}
plot(prf.ridge.test, col = "red", main='Figure 6: ROC plot for ridge logistic regression model')
lines(x = c(0,1), y = c(0,1),col="black")
text(0.5, 0.3, paste0("AUC = ",round(ridge.auc, 2)),
     cex=0.65, pos=3,col="red") 
```



```{r ResultAll2}
# display results
kable(pred_metrics, caption='Table 3', 
      digits=3, format='html', row.names=FALSE)
```

<br>

```{r, fig.cap="Figure 7: Confusion matrices for all three models predicted on test set"}

# function to create data frames w/ confusion matrix vals
create_conf_df <- function(pred, actual) {
  dat = data.frame(Predicted = c('no', 'yes', 'no', 'yes'),
                   Actual = c('no', 'no', 'yes', 'yes'),
                   props = c(mean(pred=='no' & actual=='no'),
                             mean(pred=='yes' & actual=='no'),
                             mean(pred=='no' & actual=='yes'),
                             mean(pred=='yes' & actual=='yes')))
  dat$props = round(dat$props, 3)
  return(dat)
}

# create confusion matrix df for each model on test data
conf.ridge = create_conf_df(ridge.pred.test, dat_test$y)
conf.svm.lin = create_conf_df(svm.pred.lin.test, dat_test$y)
conf.svm.rad = create_conf_df(svm.pred.rad.test, dat_test$y)

# create ggplot objects
gg.svm.ridge = ggplot(data =  conf.ridge, mapping = aes(x = Actual, y = Predicted)) +
  geom_tile(colour = "black", fill='white') +
  geom_text(aes(label = props), vjust = 1) +
  theme(legend.position = "none") +
  labs(title = '(a) Ridge logistic regression') +
  scale_y_discrete(limits=rev)

gg.svm.lin = ggplot(data =  conf.svm.lin, mapping = aes(x = Actual, y = Predicted)) +
  geom_tile(colour = "black", fill='white') +
  geom_text(aes(label = props), vjust = 1) +
  theme(legend.position = "none") +
  labs(title = '(b) SVM with linear kernel') +
  scale_y_discrete(limits=rev)

gg.svm.rad = ggplot(data =  conf.svm.rad, mapping = aes(x = Actual, y = Predicted)) +
  geom_tile(colour = "black", fill='white') +
  geom_text(aes(label = props), vjust = 1) +
  theme(legend.position = "none") +
  labs(title = '(c) SVM with radial kernel') +
  scale_y_discrete(limits=rev)

# plot
grid.arrange(gg.svm.ridge, gg.svm.lin, gg.svm.rad, nrow=2)

```


<br>

Table 3 shows all the model performance evaluation metrics using the three methods on training and testing datasets, and Figure 7 shows the confusion matrices for the predictions of each method on the testing dataset.

From Table 3, we can see that the test set F1 score for ridge logistic regression and radial kernel SVM are similar (0.354 and 0.357, respectively), while the F1 score for linear kernel SVM is slightly lower (0.326). However, examining the confusion matrices in Figure 7 shows that although ridge logistic regression and radial kernel SVM gave similar F1 scores, the predictions of each method were different. Radial SVM predicted more observations to be successes than the ridge logistic regression model. Additionally, radial SVM had higher sensitivity (0.626 vs. 0.398) while ridge logistic regression had higher specificity (0.887 vs. 0.752). 


### Variables of interest

Table 4 shows the estimated coefficients for each predictor from the ridge logistic regression model. According to the model, there were several variables contributing to the success of telemarketing. The strongest predictor, unsurprisingly, is the outcome of a previous telemarketing campaign ($\hat{\beta}=1.97$), which means a client is much more likely to make a deposit if the client made a deposit in the previous telemarketing campaign, all else equal. This finding suggests that targeting existing client is the most efficient way of telemarketing in our analysis. The second most predictive variable is job type. In our analysis, we found clients who are entrepreneurs were less likely to make a deposit ($\hat{\beta}=-1.0$). On the contrary, retired clients were more likely to make a deposit ($\hat{\beta}=0.35$). Interestingly, married individuals, home owners, and individuals with loans were less likely to contribute a deposit ($\hat{\beta}=-0.64$), ($\hat{\beta}=-0.74$) and ($\hat{\beta}=-0.51$) respectively, all else equal. 


```{r regressionCoefficient2}

# create coefficient table
coef_table = tibble(Group = c('age', 'job',
                              rep('', 10),
                              'marital', '', 'education',
                              rep('', 2), 'default', 
                              'balance', 'housing', 'loan',
                              'contact', '', 'day', 'month',
                              '', '', 'campaign', 'previous',
                              'poutcome', '', ''),
                    Covariate = row.names(model.ridge$coefficients),
                    Estimate = model.ridge$coefficients[,1])

# clean up covariate column
coef_table$Covariate = gsub('job|marital|education|contact|month|poutcome', '', coef_table$Covariate)
coef_table = coef_table %>%
  mutate(Covariate = case_when(Covariate=='10_11_12' ~ 'q4',
                               Covariate=='7_8_9' ~ 'q3',
                               Covariate=='4_5_6' ~ 'q2',
                               TRUE ~ Covariate))

# display results
kable(coef_table, caption='Table 4: Coefficients of ridge logistic regression model', 
      digits=2, format='html', row.names=FALSE)

```


## Discussion

All three methods yielded roughly equivalent F1 scores. When comparing secondary metrics, the two SVM models again performed roughly similar while logistic regression performed worse in some metrics but better than others. For example, noting again that F1 score is the harmonic mean between precision and recall, we observed that the two SVM models had substantially higher recall but slightly lower precision than the ridge logistic regression model (note that the harmonic mean up-weights the importance of the smallest value, compared to the arithmetic mean). 

The raw mis-classification error for all models was also lower than the naive model of predicting failure for all clients (12 percent). This observation points to how optimizing for different metrics can yield very different conclusions as to what constitutes the best model. 

For example, if we imagine that the bank under study desired a prediction tool which could screen out individuals who were very unlikely to subscribe a term deposit. In this case, they would want to minimize the number of false negatives while maximizing the number of true negatives. Equivalently, this would mean maximizing the negative predictive value. We could also imagine the bank wanting a small list of clients to target but also wanting to not leave out any individuals who would actually subscribe a term deposit. Then, this would be equivalent to maximizing sensitivity, or the ratio of true positives to false negatives. 

The ridge logistic regression is able to quantitatively assess the association of each predictor with the subscription outcome, which may suggest ways on how to implement the next telemarketing campaign. First, individuals who previously subscribed should be targeted again in the new campaign. Second, telemarketing may benefit from targeting retirees, the unmarried, non-homeowners, and loan-free individuals. 

Finally, we note two limitations of any prediction model using this dataset. First, applying models based on historical data to future populations requires an assumption of transportability. If the future population is substantially different from the historical population the model is based on, in the sense that their relationship to the outcome variable has changed, then that would limit the applicability of the model. For example, consider if this data was collected during a period of general confidence in banks and financial systems. Now imagine an event happens after the data is collected such as a recession or banking failure which reduces confidence in banks for many individuals. If confidence in banks is important for whether or not an individuals subscribes a term deposit, and if there is no way to predict this variable, then models trained on the historical data would have limited utility.

Second, we note the limitation that the outcome variable is binary, when in practice, it is likely that banks would care about the amount which an individual deposits. 


## References